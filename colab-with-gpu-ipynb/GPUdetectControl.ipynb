{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EclrHSbzPF8G",
        "outputId": "b55090cf-40d1-48ee-bd6e-da9f7018f6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Tensorflow.\n",
            "\n",
            "\n",
            "[####]    ----  PROJECT:  GPU USAGE - DISTRIBUTION STRATEGIES - FINE-GRAINED CONTROL  ----\n",
            "\n",
            "\n",
            "[####]    Tensorflow version: 2.8.2  -  Keras version: 2.8.0\n",
            "\n",
            "[####]    Tensorflow logger initial effective logging level: 30\n",
            "\n",
            "[####]    Number of available GPUs: 1\n",
            "\n",
            "[####]    Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "[####]    Turning on device placement logging so we can see GPU/CPU assignment. (tf.debugging)\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "[####]    Tensors forced onto CPU. MatMul operation will run on GPU if possible.\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "[####]    Restrict TensorFlow to only use the first GPU.\n",
            "\n",
            "[####]    Physical GPU count: 1    Logical GPU count: 1\n",
            "\n",
            "[####]    Limiting GPU memory growth: Setting experimental memory growth control to True on GPUs.\n",
            "\n",
            "[####]    Setting experimental memory growth control to True on GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "\n",
            "[####]    *** EXCEPTION ***: RuntimeError\n",
            "Physical devices cannot be modified after being initialized\n"
          ]
        }
      ],
      "source": [
        "#! /usr/bin/env python\n",
        "# git@github.com:jimmygizmo/tensorpic/tf-image-classification-flowers.py\n",
        "# Version 0.5.0\n",
        "\n",
        "print(\"Initializing Tensorflow.\")\n",
        "import tensorflow as tf\n",
        "import pprint\n",
        "\n",
        "\n",
        "# This program was inspired by the following Tensorflow tutorial. Some text was copied verbatim into the comments.\n",
        "# https://www.tensorflow.org/guide/gpu\n",
        "\n",
        "# Related guide: Optimize TensorFlow GPU Performance\n",
        "# https://www.tensorflow.org/guide/gpu_performance_analysis\n",
        "\n",
        "CONSTANT = \"blah\"\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"\\n[####]    {msg}\")\n",
        "\n",
        "\n",
        "def log_phase(msg):\n",
        "    print(f\"\\n\\n[####]    ----  {msg}  ----\\n\")\n",
        "\n",
        "\n",
        "log_phase(f\"PROJECT:  GPU USAGE - DISTRIBUTION STRATEGIES - FINE-GRAINED CONTROL\")\n",
        "\n",
        "log(f\"Tensorflow version: {tf.__version__}  -  Keras version: {tf.keras.__version__}\")\n",
        "tf_logger_initial_level = tf.get_logger().getEffectiveLevel()\n",
        "log(f\"Tensorflow logger initial effective logging level: {tf_logger_initial_level}\")\n",
        "\n",
        "available_gpus = tf.config.list_physical_devices('GPU')\n",
        "available_gpu_count = len(available_gpus)\n",
        "log(f\"Number of available GPUs: {available_gpu_count}\")\n",
        "log(f\"Available GPUs: {available_gpus}\")\n",
        "\n",
        "# \"/device:CPU:0\": The CPU of your machine.\n",
        "# \"/GPU:0\": Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n",
        "# \"/job:localhost/replica:0/task:0/device:GPU:1\": Fully qualified name of the second GPU of\n",
        "#     your machine that is visible to TensorFlow.\n",
        "\n",
        "log(f\"Turning on device placement logging so we can see GPU/CPU assignment. (tf.debugging)\")\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Create some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "print(c)\n",
        "\n",
        "# The following code will force certain operations on the CPU, whereas they would have otherwise\n",
        "# defaulted to the GPU. The MatMul should default to any available GPU.\n",
        "\n",
        "log(f\"Tensors forced onto CPU. MatMul operation will run on GPU if possible.\")\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Place tensors on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Tensors will be automatically copied between devices if required.\n",
        "\n",
        "# Run on the GPU\n",
        "c = tf.matmul(a, b)\n",
        "print(c)\n",
        "\n",
        "\n",
        "# By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible\n",
        "# to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices\n",
        "# by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs, use the tf.config.set\n",
        "# visible_devices method.\n",
        "log(f\"Restrict TensorFlow to only use the first GPU.\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only use the first GPU\n",
        "    try:\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        log(f\"Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow\n",
        "# the memory usage as is needed by the process. TensorFlow provides two methods to control this.\n",
        "\n",
        "# The first option is to turn on memory growth by calling tf.config.experimental.set_memory_growth, which attempts\n",
        "# to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little\n",
        "# memory, and as the program gets run and more GPU memory is needed, the GPU memory region is extended for the\n",
        "# TensorFlow process. Memory is not released since it can lead to memory fragmentation. To turn on memory growth\n",
        "# for a specific GPU, use the following code prior to allocating any tensors or executing any ops.\n",
        "\n",
        "log(f\"Limiting GPU memory growth: Setting experimental memory growth control to True on GPUs.\")\n",
        "# TODO: Clarify, does it touch all GPUs or only those \"visible\" as per above. Look closer at list_physical_devices.\n",
        "#   Need a multi-GPU environment to test this.\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            log(f\"Setting experimental memory growth control to True on GPU: {gpu}\")\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "            log(f\"Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}