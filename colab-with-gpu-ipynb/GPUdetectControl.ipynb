{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#! /usr/bin/env python\n",
        "# git@github.com:jimmygizmo/tensorpic/tf-image-classification-flowers.py\n",
        "# Version 0.5.0\n",
        "\n",
        "print(\"Initializing Tensorflow.\")\n",
        "import tensorflow as tf\n",
        "import pprint\n",
        "\n",
        "\n",
        "# This program was inspired by the following Tensorflow tutorial. Some text was copied verbatim into the comments.\n",
        "# https://www.tensorflow.org/guide/gpu\n",
        "\n",
        "# Related guide: Optimize TensorFlow GPU Performance\n",
        "# https://www.tensorflow.org/guide/gpu_performance_analysis\n",
        "\n",
        "CONSTANT = \"blah\"\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"\\n[####]    {msg}\")\n",
        "\n",
        "\n",
        "def log_phase(msg):\n",
        "    print(f\"\\n\\n[####]    ----  {msg}  ----\\n\")\n",
        "\n",
        "\n",
        "log_phase(f\"PROJECT:  GPU USAGE - DISTRIBUTION STRATEGIES - FINE-GRAINED CONTROL\")\n",
        "\n",
        "log(f\"Tensorflow version: {tf.__version__}  -  Keras version: {tf.keras.__version__}\")\n",
        "tf_logger_initial_level = tf.get_logger().getEffectiveLevel()\n",
        "log(f\"Tensorflow logger initial effective logging level: {tf_logger_initial_level}\")\n",
        "\n",
        "available_gpus = tf.config.list_physical_devices('GPU')\n",
        "available_gpu_count = len(available_gpus)\n",
        "log(f\"Number of available GPUs: {available_gpu_count}\")\n",
        "log(f\"Available GPUs: {available_gpus}\")\n",
        "\n",
        "# \"/device:CPU:0\": The CPU of your machine.\n",
        "# \"/GPU:0\": Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n",
        "# \"/job:localhost/replica:0/task:0/device:GPU:1\": Fully qualified name of the second GPU of\n",
        "#     your machine that is visible to TensorFlow.\n",
        "\n",
        "log(f\"Turning on device placement logging so we can see GPU/CPU assignment. (tf.debugging)\")\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Create some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "print(c)\n",
        "\n",
        "# The following code will force certain operations on the CPU, whereas they would have otherwise\n",
        "# defaulted to the GPU. The MatMul should default to any available GPU.\n",
        "\n",
        "log(f\"Tensors forced onto CPU. MatMul operation will run on GPU if possible.\")\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Place tensors on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Tensors will be automatically copied between devices if required.\n",
        "\n",
        "# Run on the GPU\n",
        "c = tf.matmul(a, b)\n",
        "print(c)\n",
        "\n",
        "\n",
        "# By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible\n",
        "# to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices\n",
        "# by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs, use the tf.config.set\n",
        "# visible_devices method.\n",
        "log(f\"Restrict TensorFlow to only use the first GPU.\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only use the first GPU\n",
        "    try:\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        log(f\"Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow\n",
        "# the memory usage as is needed by the process. TensorFlow provides two methods to control this.\n",
        "\n",
        "# The first option is to turn on memory growth by calling tf.config.experimental.set_memory_growth, which attempts\n",
        "# to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little\n",
        "# memory, and as the program gets run and more GPU memory is needed, the GPU memory region is extended for the\n",
        "# TensorFlow process. Memory is not released since it can lead to memory fragmentation. To turn on memory growth\n",
        "# for a specific GPU, use the following code prior to allocating any tensors or executing any ops.\n",
        "\n",
        "log(f\"Limiting GPU memory growth: Setting experimental memory growth control to True on GPUs.\")\n",
        "# TODO: Clarify, does it touch all GPUs or only those \"visible\" as per above. Look closer at list_physical_devices.\n",
        "#   Need a multi-GPU environment to test this.\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            log(f\"Setting experimental memory growth control to True on GPU: {gpu}\")\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            log(f\"Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "# Another way to enable this option is to set the environmental variable TF_FORCE_GPU_ALLOW_GROWTH to true.\n",
        "# This configuration is platform specific.\n",
        "\n",
        "# The second method is to configure a virtual GPU device with tf.config.set_logical_device_configuration and set\n",
        "# a hard limit on the total memory to allocate on the GPU.\n",
        "# This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process.\n",
        "# This is common practice for local development when the GPU is shared with other applications such as a\n",
        "# workstation GUI.\n",
        "\n",
        "log(f\"Configure a virtual GPU device. Set a hard limit on the total memory to allocate to the GPU.\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "    try:\n",
        "        tf.config.set_logical_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.LogicalDeviceConfiguration(memory_limit=1024)]\n",
        "        )\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        log(f\"Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Virtual devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# DISABLING INTENTIONAL ERROR CASE.\n",
        "# log(f\"Use a single GPU on a multi-GPU system. - ERROR Example - invalid device.\")\n",
        "# # If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default.\n",
        "# # If you would like to run on a different GPU, you will need to specify the preference explicitly:\n",
        "# tf.debugging.set_log_device_placement(True)\n",
        "# try:\n",
        "#     # Specify an invalid GPU device\n",
        "#     with tf.device('/device:GPU:2'):\n",
        "#         a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "#         b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "#         c = tf.matmul(a, b)\n",
        "# except RuntimeError as e:\n",
        "#     #log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "#     print(e)\n",
        "#\n",
        "# # TODO: Try to understand why the above intentional-error code (invalid device, GPU:2) did not error on my Mac.\n",
        "# # Perhaps there is new behavior. It seems to have happily chosen the available CPU:0\n",
        "# 2022-10-07 19:55:23.473319: I tensorflow/core/common_runtime/eager/execute.cc:1419] Executing op _EagerConst in\n",
        "#   device/job:localhost/replica:0/task:0/device:CPU:0\n",
        "# 2022-10-07 19:55:23.473741: I tensorflow/core/common_runtime/eager/execute.cc:1419] Executing op _EagerConst in\n",
        "#   device /job:localhost/replica:0/task:0/device:CPU:0\n",
        "# 2022-10-07 19:55:23.474464: I tensorflow/core/common_runtime/eager/execute.cc:1419] Executing op MatMul in\n",
        "#   device /job:localhost/replica:0/task:0/device:CPU:0\n",
        "\n",
        "\n",
        "# If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case\n",
        "#   the specified one doesn't exist, you can call tf.config.set_soft_device_placement(True).\n",
        "log(f\"Automatically choose an existing and supported device. set_soft_device_placement(True)\")\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "tf.config.set_soft_device_placement(True)\n",
        "\n",
        "# Creates some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)\n",
        "\n",
        "\n",
        "# Using multiple GPUs. Developing for multiple GPUs will allow a model to scale with the additional resources.\n",
        "# If developing on a system with a single GPU, you can simulate multiple GPUs with virtual devices.\n",
        "# This enables easy testing of multi-GPU setups without requiring additional resources.\n",
        "log(f\"Simulate multiple GPUs to enable development/testing for multi-GPU systems.\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Create 2 virtual GPUs with 1GB memory each\n",
        "    try:\n",
        "        tf.config.set_logical_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
        "             tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        log(f\"AFTER MULTI VIRTUAL ADDED - Physical GPU count: {len(gpus)}    Logical GPU count: {len(logical_gpus)}\")\n",
        "    except RuntimeError as e:\n",
        "        log(f\"*** EXCEPTION ***: RuntimeError\")\n",
        "        # Virtual devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "log(f\"SIMULATE MULTIPLE GPUs. Notice above, two virtual GPUs were created. ** REQUIRES A PHYSICAL GPU TO WORK.\")\n",
        "# UPDATE: We get the exception with or without a GPU.\n",
        "\n",
        "# * * * * * * * * * * * * * * * * *\n",
        "# There appears to be some issue. A lot of these blocks above are getting an error I cannot yet explain.\n",
        "# Physical devices cannot be modified after being initialized\n",
        "# This happens for both GPU present and not present\n",
        "# * * * * * * * * * * * * * * * * *\n",
        "\n",
        "# TODO: The last two small blocks of code still need to be tried from the tutorial at:\n",
        "# https://www.tensorflow.org/guide/gpu\n",
        "# TODO: The above exception case should not be happening as much or maybe not at all and the issue is not\n",
        "#   yet understood.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlIOgiiTxH2_",
        "outputId": "67a7ab4e-be3f-450e-a65b-42103ae2cf18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Tensorflow.\n",
            "\n",
            "\n",
            "[####]    ----  PROJECT:  GPU USAGE - DISTRIBUTION STRATEGIES - FINE-GRAINED CONTROL  ----\n",
            "\n",
            "\n",
            "[####]    Tensorflow version: 2.8.2  -  Keras version: 2.8.0\n",
            "\n",
            "[####]    Tensorflow logger initial effective logging level: 30\n",
            "\n",
            "[####]    Number of available GPUs: 1\n",
            "\n",
            "[####]    Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "[####]    Turning on device placement logging so we can see GPU/CPU assignment. (tf.debugging)\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "[####]    Tensors forced onto CPU. MatMul operation will run on GPU if possible.\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "[####]    Restrict TensorFlow to only use the first GPU.\n",
            "\n",
            "[####]    Physical GPU count: 1    Logical GPU count: 1\n",
            "\n",
            "[####]    Limiting GPU memory growth: Setting experimental memory growth control to True on GPUs.\n",
            "\n",
            "[####]    Setting experimental memory growth control to True on GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "\n",
            "[####]    *** EXCEPTION ***: RuntimeError\n",
            "Physical devices cannot be modified after being initialized\n",
            "\n",
            "[####]    Configure a virtual GPU device. Set a hard limit on the total memory to allocate to the GPU.\n",
            "\n",
            "[####]    *** EXCEPTION ***: RuntimeError\n",
            "Virtual devices cannot be modified after being initialized\n",
            "\n",
            "[####]    Automatically choose an existing and supported device. set_soft_device_placement(True)\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "[####]    Simulate multiple GPUs to enable development/testing for multi-GPU systems.\n",
            "\n",
            "[####]    *** EXCEPTION ***: RuntimeError\n",
            "Virtual devices cannot be modified after being initialized\n",
            "\n",
            "[####]    SIMULATE MULTIPLE GPUs. Notice above, two virtual GPUs were created. ** REQUIRES A PHYSICAL GPU TO WORK.\n"
          ]
        }
      ]
    }
  ]
}